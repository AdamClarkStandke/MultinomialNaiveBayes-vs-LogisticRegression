# Comparing an Additive Smoothing Multinomial Naive-Bayes classifier versus Logistic Regression for Detecting fake news 

Implemented a fake news classifier as detailed by Katharine Jarmul in her Introduction to Natural Language Processing in Python course on DataCamp and by George McIntire as detaled in his article at [kdnuggets](https://www.kdnuggets.com/2017/04/machine-learning-fake-news-accuracy.html) 

Dataset was collected from lutzhame's repository found at [lutzhamel](https://github.com/lutzhamel/fake-news/blob/master/data/fake_or_real_news.csv) 

Additionally, to sklearn's logistic regression classifer, I implemented my own logistic regression classifier as detailed by Suraj Verma in his Logistic Regression From Scratch in Python course found at [Logistic Regression From Scratch](https://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2) and by Dr. Mirko StojiljkoviÄ‡ in his Stochastic Gradient Descent Algorithm With Python as found at [Stochastic Gradient Descent Algorithm](https://realpython.com/gradient-descent-algorithm-python/).

I also implemented a non-metaheurstic simulation method for finding a good solution to the additive smoothing parameter for sklearn's Multinomial Naive Bayes classifier. 

Code and analysis can be found in the following file: [code](https://github.com/aCStandke/SimpleFakeOrRealClassifier/blob/main/Building%20a%20%22fake%20news%22%20classifier.ipynb)

# Results:

  * ## Confusion Matrix of Multinomial Naive-Bayes Classifier
  ![]()


  * ##  Confusion Matrix of Additive Smoothing Multinomial Naive-Bayes Classifier
   ![]()

  * ## Confusion Matrix of Logistic Regression
   ![]()
